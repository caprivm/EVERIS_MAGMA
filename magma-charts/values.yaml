# Copyright 2020 The Magma Authors.

# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

## Global values for NMS sub-chart
nms:
  enabled: true
  secret:
    certs: orc8r-secrets-certs

  magmalte:
    create: true

    manifests:
      secrets: true
      deployment: true
      service: true
      rbac: true

    env:
      api_host: orc8r-proxy:9443
      host: 0.0.0.0
      port: 8081
      mapbox_access_token: ""
      mysql_host: mariadb.mariadb.svc.cluster.local
      mysql_db: magma
      mysql_user: root
      mysql_pass: magma
      mysql_dialect: mariadb

    image:
      repository: docker.io/caprivm/magmalte
      tag: 1.4.0
      pullPolicy: IfNotPresent
    
    deployment:
      spec:
        operator_cert_name: admin_operator.pem
        operator_key_name: admin_operator.key.pem
    
    service:
      type: LoadBalancer
      http:
        port: 8081
        targetport: 8081
        nodePort: ""
    
    replicas: 1
  
  nginx:
    create: true
    labels: {}
    
    manifests:
      configmap: true
      secrets: true
      deployment: true
      service: true
      rbac: true

    image:
      repository: nginx
      tag: latest
      pullPolicy: IfNotPresent
    
    service:
      type: LoadBalancer
      annotations: {}
      https:
        port: 443
        targetport: 443
        nodePort: ""
    
    deployment:
      spec:
        ssl_cert_name: controller.crt
        ssl_cert_key_name: controller.key

    replicas: 1

# Reference to one or more secrets to be used when pulling images
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# - name: orc8r-secrets-registry

# Metrics sub-chart configuration.
metrics:
  enabled: true

  metrics:
    volumes:
      prometheusConfig:
        volumeSpec:
          persistentVolumeClaim:
            claimName: promcfg
      prometheusData:
        volumeSpec:
          persistentVolumeClaim:
            claimName: promdata

  grafana:
    create: false
    # Service configuration.
    service:
      annotations: {}
      labels: {}
      type: LoadBalancer
      ports:
        - name: grafana
          port: 3000
          targetPort: 3000
    
    environment:
      prometheusHost: "orc8r-prometheus"
      prometheusPort: "9090"

    image:
      repository: docker.io/grafana/grafana
      tag: latest
      pullPolicy: IfNotPresent

  userGrafana:
    create: true
    # Service configuration.
    service:
      annotations: {}
      labels: {}
      type: LoadBalancer
      ports:
        - name: grafana
          port: 3000
          targetPort: 3000

    environment:
      prometheusHost: "orc8r-prometheus"
      prometheusPort: "9090"

    image:
      repository: grafana/grafana
      tag: latest
      pullPolicy: IfNotPresent
    
    volumes:
      # Default volume configurations for grafana data.
      dashboards:
        persistentVolumeClaim:
          claimName: grafanadashboards
      datasources:
        persistentVolumeClaim:
          claimName: grafanadatasources
      dashboardproviders:
        persistentVolumeClaim:
          claimName: grafanaproviders
      grafanaData:
        persistentVolumeClaim:
          claimName: grafanadata

  prometheus:
    create: true

    # Preconfigure alerts for orchestrator in prometheus
    includeOrc8rAlerts: false

    prometheusCacheHostname: "orc8r-prometheus-cache"
    alertmanagerHostname: "orc8r-alertmanager"

    service:
      annotations: {}
      labels: {}
      type: ClusterIP
      ports:
        - name: prometheus
          port: 9090
          targetPort: 9090

    resources: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}

    customAlerts: []

    image:
      repository: docker.io/prom/prometheus
      tag: v2.20.1
      pullPolicy: IfNotPresent

    retention:
      time: 7d

    replicas: 1

  prometheusCache:
    create: true
    service:
      annotations: {}
      labels: {}
      type: ClusterIP
      ports:
        - name: orc8r-prometheus-cache
          port: 9091
          targetPort: 9091

    image:
      repository: docker.io/facebookincubator/prometheus-edge-hub
      tag: 1.0.0
      pullPolicy: IfNotPresent

    # Maximum number of datapoints in the cache at one time. Unlimited if <= 0.
    limit: 0
    replicas: 1

  alertmanager:
    create: false
    service:
      annotations: {}
      labels: {}
      type: ClusterIP
      ports:
        - name: alertmanager
          port: 9093
          targetPort: 9093

    image:
      repository: docker.io/prom/alertmanager
      tag: v0.18.0
      pullPolicy: IfNotPresent

    resources: {}
    nodeSelector: {}
    tolerations: []
    # Pod affinity must be used to ensure that this pod runs on same node as prometheus
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                    - prometheus
            topologyKey: "kubernetes.io/hostname"

    replicas: 1

  alertmanagerConfigurer:
    # Enable/Disable chart
    create: false
    image:
      repository: docker.io/prom/alertmanager
      tag: v0.18.0
      pullPolicy: IfNotPresent

  prometheusConfigurer:
    # Enable/Disable chart
    create: true
    # Service configuration.
    service:
      annotations: {}
      labels: {}
      type: ClusterIP
      ports:
        - name: prom-configmanager
          port: 9100
          targetPort: 9100
        - name: alertmanager-config
          port: 9101
          targetPort: 9101

    environment:
      promAlertconfigPort: "9100"
      rulesDir: "/etc/configs/alert_rules"
      prometheusURL: "orc8r-prometheus:9090"
      alertmanagerConfigPort: "9101"
      alertmanagerConfPath: "/etc/configs/alertmanager.yml"
      alertmanagerURL: "orc8r-alertmanager:9093"


    image:
      repository: docker.io/facebookincubator/prometheus-configurer
      tag: 1.0.0
      pullPolicy: IfNotPresent

    resources: {}
    nodeSelector: {}
    tolerations: []
    # Pod affinity must be used to ensure that this pod runs on same node as prometheus
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                    - prometheus
            topologyKey: "kubernetes.io/hostname"

    replicas: 1

#   thanos:
#     enabled: false

# Secrets sub-chart configuration.
secrets:
  create: true

# Define which secrets should be mounted by pods.
secret:
  certs: orc8r-secrets-certs
  configs:
    orc8r: orc8r-secrets-configs-orc8r
  envdir: orc8r-secrets-envdir

proxyserviceloadBalancerIP: ""
nginxserviceloadBalancerIP: ""

proxy:
  # Service configuration.
  service:
    name: bootstrapper-orc8r-nginx
    annotations: {}
    labels: {}
    type: LoadBalancer
    port:
      clientcert:
        port: 7443
        targetPort: 8443
        nodePort: ""
      open:
        port: 7444
        targetPort: 8444
        nodePort: ""
      api:
        port: 9443
        targetPort: 9443
        nodePort: ""
      health:
        port: 80
        targetPort: 80
        nodePort: ""
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

  # nginx image
  image:
    repository: docker.io/caprivm/nginx
    tag: 1.4.0
    pullPolicy: IfNotPresent

  # Settings affecting nginx application
  spec:
    # magma controller domain name
    hostname: "controller.magma.test"
    http_proxy_backend: "orc8r-controller"
    # when nginx sees a variable in a server_name it needs a resolver
    # by default we'll use kube-dns
    resolver: "coredns.kube-system.svc.cluster.local valid=10s"

  # Number of nginx replicas desired
  replicas: 1

  # Resource limits & requests
  resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

  # Define which Nodes the Pods are scheduled on.
  # ref: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  # Tolerations for use with node taints
  # ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  # Assign nginx to run on specific nodes
  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  affinity: {}

controller:
  # Service configuration.
  service:
    annotations: {}
    labels: {}
    type: ClusterIP
    port: 8080
    targetPort: 8080
    # port range exposed by controller
    portStart: 9079
    portEnd: 9108

  # Controller image
  image:
    repository: docker.io/caprivm/controller
    tag: 1.4.0
    pullPolicy: IfNotPresent
  
  spec:
    # Postgres/mysql configuration
    database:
      driver: postgres  # mysql/postgres
      sql_dialect: psql # maria/psql
      db: magma         # DB Name
      protocol: tcp
      host: postgresql
      port: 5432
      user: postgres
      pass: postgres
  
  migration:
    new_handlers: 0
    new_mconfigs: 0
    mconfig_whitelist: ""

  # Number of controller replicas desired
  replicas: 1

  # Resource limits & requests
  resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

  # Define which Nodes the Pods are scheduled on.
  # ref: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  # Tolerations for use with node taints
  # ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  # Assign proxy to run on specific nodes
  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  affinity: {}

# Set True to create a CloudWatch agent to monitor metrics
cloudwatch:
  create: false

postgresql:
  create: true

# logging sub-chart configuration.
logging:
  enabled: true