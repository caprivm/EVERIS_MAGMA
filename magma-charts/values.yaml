# Copyright 2020 The Magma Authors.

# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

## Global values for NMS sub-chart
nms:
  enabled: true
  secret:
      certs: orc8r-secrets-certs
  
  magmalte:
    create: true
    replicas: 1
    
    env:
      api_host: orc8r-nginx-proxy.magma.svc.cluster.local:443
      host: 0.0.0.0
      port: 8081
      mapbox_access_token: ""
      mysql_host: mysql2-mariadb.magma.svc.cluster.local
      mysql_db: magma
      mysql_user: root
      mysql_pass: iNAReV6C6J
      mysql_dialect: mariadb
      grafana_address: orc8r-user-grafana:3000

    image:
      repository: docker.io/caprivm/magmalte
      tag: 1.3.2
      pullPolicy: IfNotPresent
    
    deployment:
      spec:
        operator_cert_name: admin_operator.pem
        operator_key_name: admin_operator.key.pem

    service:
      type: ClusterIP
      http:
        port: 8081
        targetport: 8081
        nodePort: ""

  nginx:
    create: true
    replicas: 1
    image:
      repository: nginx
      tag: latest
      pullPolicy: Always
  
    service:
      type: LoadBalancer
      annotations: {}
      https:
        port: 443
        targetport: 443
        nodePort: ""

    metadata:
      annotations:
        # nginx.org/proxy-connect-timeout: "180s"
        # nginx.org/proxy-read-timeout: "180s"

    deployment:
      spec:
        ssl_cert_name: nms_nginx.pem
        ssl_cert_key_name: nms_nginx.key.pem

# Reference to one or more secrets to be used when pulling images
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# - name: orc8r-secrets-registry

# Metrics sub-chart configuration.
metrics:
  enabled: false

  metrics:
    create: true
    volumes:
      prometheusConfig:
        volumeSpec:
          hostPath:
            path: /prometheusData
            type: DirectoryOrCreate
        # persistentVolumeClaim:
          # claimName: promcfg
      prometheusData:
        volumeSpec:
          hostPath:
            path: /configs/prometheus
            type: DirectoryOrCreate
        # persistentVolumeClaim:
          # claimName: promdata

  grafana:
    create: false
   
  userGrafana:
    create: true 
    image:
      repository: grafana/grafana
      tag: 6.6.2
      pullPolicy: IfNotPresent

    volumes:
      # Default volume configurations for grafana data.
      dashboards:
        hostPath:
          path: /grafanaData/dashboards
          type: DirectoryOrCreate
              #volumeSpec:
                # persistentVolumeClaim:
                #claimName: grafanadashboards
      datasources:
        hostPath:
          path: /grafanaData/datasources
          type: DirectoryOrCreate
              # volumeSpec:
                #persistentVolumeClaim:
                #claimName: grafanadatasources
      dashboardproviders:
        hostPath:
          path: /grafanaData/dashboardproviders
          type: DirectoryOrCreate
              #volumeSpec:
                #persistentVolumeClaim:
                #claimName: grafanaproviders
      grafanaData:
        hostPath:
          path: /grafanaData/grafanaData
          type: DirectoryOrCreate
              #volumeSpec:
              #persistentVolumeClaim:
              #claimName: grafanadata

  prometheus:
    create: true
    image:
      repository: prom/prometheus
      tag: v2.20.1
      pullPolicy: IfNotPresent


  prometheusCache:
    create: true
    image:
      repository: docker.io/facebookincubator/prometheus-edge-hub
      tag: 1.1.0
      pullPolicy: IfNotPresent
    # Maximum number of datapoints in the cache at one time. Unlimited if <= 0.
    limit: 500000
    service:
      type: LoadBalancer

  alertmanager:
    create: true

  alertmanagerConfigurer:
    create: true
    image:
      repository: docker.io/facebookincubator/alertmanager-configurer
      tag: 1.0.0
      pullPolicy: IfNotPresent

  prometheusConfigurer:
    create: true
    image:
      repository: docker.io/facebookincubator/prometheus-configurer
      tag: 1.0.0
      pullPolicy: IfNotPresent

# Secrets sub-chart configuration.
secrets:
  create: true

# Define which secrets should be mounted by pods.
secret:
  certs: orc8r-secrets-certs
  configs:
    orc8r: orc8r-secrets-configs-orc8r
  envdir: orc8r-secrets-envdir

nginx:
  # Enable/Disable chart
  create: true
  
  # Service configuration.
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  # Service configuration.
  service:
    enabled: true
    legacyEnabled: true
    name: bootstrapper-orc8r-nginx
    annotations: {}
    labels: {}
    type: LoadBalancer
    port:
      clientcert:
        port: 8443
        targetPort: 8443
        nodePort: ""
      open:
        port: 8444
        targetPort: 8444
        nodePort: ""
      api:
        port: 443
        targetPort: 9443
        nodePort: ""
      health:
        port: 80
        targetPort: 80
        nodePort: ""
    loadBalancerIP: "192.168.1.12"
    loadBalancerSourceRanges: []

  # nginx image
  image:
    repository: docker.io/caprivm/nginx
    tag: 1.3.2
    pullPolicy: IfNotPresent

  # Settings affecting nginx application
  spec:
    # magma controller domain name
    hostname: "orc8r" # "controller.magma.test"
    # http_proxy_backend: "orc8r-controller"
    # when nginx sees a variable in a server_name it needs a resolver
    # by default we'll use kube-dns
    resolver: "coredns.kube-system.svc.cluster.local valid=10s"

  # Number of nginx replicas desired
  replicas: 1

  # Resource limits & requests
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

controller:
  # Define which secrets should be mounted by pods.
  secret:
    certs: orc8r-secrets-certs
    configs:
      orc8r: orc8r-secrets-configs-orc8r
    envdir: orc8r-secrets-envdir
  
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  # Service configuration.
  service:
    annotations: {}
    labels: {}
    type: LoadBalancer
    port: 8080
    targetPort: 8080
    # port range exposed by controller
    portStart: 9079
    portEnd: 9108

  # Controller image
  image:
    repository: docker.io/caprivm/controller
    tag: 1.3.2
    pullPolicy: IfNotPresent
  
  spec:
    # Postgres/mysql configuration
    database:
      driver: postgres  # mysql/postgres
      sql_dialect: psql # maria/psql
      db: magma         # DB Name
      protocol: tcp
      host: postgresql
      port: 5432
      user: postgres
      pass: postgres
  
  migration:
    new_handlers: 0
    new_mconfigs: 0
    mconfig_whitelist: ""

  podAnnotations: {}

  # Number of controller replicas desired
  replicas: 1

  # Resource limits & requests
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Set True to create a CloudWatch agent to monitor metrics
cloudwatch:
  create: false

# logging sub-chart configuration.
logging:
  enabled: false

# mariadb:
  # create: false
  # enabled: false
  # global:
  #   storageClass: managed-nfs-storage
  # image:
  #   registry: docker.io
  #   repository: bitnami/mariadb
  #   tag: 10.3.22-debian-10-r44
  #   debug: true
  # master:
  #   persistence:
  #     size: 4Gi
  #   nodeSelector:
  #     kubernetes.io/hostname: compute4
  #   extraFlags: --sql-mode=ANSI_QUOTES
  # slave:
  #   persistence:
  #     size: 4Gi
  #   nodeSelector:
  #     kubernetes.io/hostname: compute4
  # db:
  #  name: magma
  #  user: magma
  # password: magma
  # forcePassword: true
  #   injectSecretsAsVolume: true
  # rootUser:
  #   password: magma
  #   forcePassword: true
  #   injectSecretsAsVolume: true
  # replication:
  #   password: magma
  #   forcePassword: true
  #   injectSecretsAsVolume: true

postgresql:
  enabled: true

elasticsearch:
  enabled: true
  replicas: 1
  minimumMasterNodes: 1

  image: "docker.elastic.co/elasticsearch/elasticsearch"
  imageTag: "7.9.3"
  imagePullPolicy: "IfNotPresent"

  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    storageClassName: managed-nfs-storage
    resources:
      requests:
        storage: 20Gi

  rbac:
    create: true
    serviceAccountAnnotations: {}
    serviceAccountName: ""

  antiAffinity: "soft"

  nodeAffinity: {}

  protocol: http
  httpPort: 9200
  transportPort: 9300

  service:
    labels: {}
    labelsHeadless: {}
    type: LoadBalancer
    nodePort: ""
    annotations:
      external-dns.alpha.kubernetes.io/hostname: elasticsearch.magma.test
    httpPortName: http
    transportPortName: transport
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    externalTrafficPolicy: ""

  # https://www.elastic.co/guide/en/elasticsearch/reference/7.9/cluster-health.html#request-params wait_for_status
  clusterHealthCheckParams: "wait_for_status=green&timeout=1s"

kibana:
  enabled: true
  image: "docker.elastic.co/kibana/kibana"
  imageTag: "7.9.4-SNAPSHOT"
  imagePullPolicy: "IfNotPresent"
  
  elasticsearchHosts: "http://elasticsearch-master:9200"

  replicas: 1

  extraEnvs:
    - name: "LOGGING_VERBOSE"
      value: "false"

  protocol: http
  serverHost: "0.0.0.0"
  healthCheckPath: "/app/kibana"

  service:
    type: LoadBalancer
    port: 5601
    nodePort: ""
    labels: {}
    annotations: {}
    httpPortName: http

  kibanaConfig:
    kibana.yml: |
      ## Default Kibana configuration from kibana-docker.
      server.name: "orc8r-kibana"
      server.port: 5601
      server.host: "0.0.0.0"
      elasticsearch.hosts: "http://elasticsearch-master:9200"
      elasticsearch.requestTimeout: 90000

  dashboardImport:
    enabled: true
    timeout: 60
    dashboards:
      k8s: https://raw.githubusercontent.com/monotek/kibana-dashboards/master/k8s-fluentd-elasticsearch.json

